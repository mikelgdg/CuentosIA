import streamlit as st
import google.generativeai as genai
import json
import os
import random
from rotacion_claves import get_api_rotator

# Configuraci√≥n de p√°gina mejorada
st.set_page_config(
    page_title="AIfil",
    page_icon=".streamlit/logo2.png",  # Usar tu logo como icono de la pesta√±a
    layout="centered",
    initial_sidebar_state="expanded"
)

# Cargar CSS personalizado
def load_css():
    """Carga el CSS personalizado"""
    css_file = ".streamlit/_style.css"
    if os.path.exists(css_file):
        with open(css_file) as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

load_css()

# Obtener el rotador de claves API
api_rotator = get_api_rotator()

# --- Funciones de carga de base y procesamiento de texto (adaptadas de cuentos.py) ---
def cargar_base(path="base_textos.json"):
    """Carga la base de textos desde un archivo JSON."""
    if not os.path.exists(path):
        # Crear un archivo base_textos.json de ejemplo si no existe
        # Esto es crucial para que la aplicaci√≥n no falle al iniciar si el usuario no tiene el archivo.
        ejemplo_base = [
            {
                "titulo": "El Arte de la Guerra",
                "autor": "Sun Tzu",
                "texto": "El arte de la guerra se basa en el enga√±o. Por lo tanto, cuando eres capaz de atacar, debes parecer incapaz; cuando las tropas se mueven, deben parecer inactivas. Cuando est√°s cerca del enemigo, debes hacerle creer que est√°s lejos; cuando est√°s lejos, debes hacerle creer que est√°s cerca. Pon cebos para atraer al enemigo. Simula desorden y apl√°stalo. Si el enemigo tiene buenas defensas, prep√°rate para la batalla. Si tiene una fuerza superior, ev√≠tala. Si tu oponente es iracundo, intenta irritarlo. Si es arrogante, trata de animar su vanidad. Si est√° en reposo, hazlo trabajar. Si su unidad es armoniosa, div√≠dela. Ataca donde el enemigo no est√° preparado; aparece donde no te esperan. Estas son las claves de la victoria para el estratega."
            },
            {
                "titulo": "Meditaciones",
                "autor": "Marco Aurelio",
                "texto": "No te afanes por las cosas exteriores. Oc√∫pate de ti mismo. Los hombres existen los unos para los otros; instruye, pues, o soporta. El hombre est√° hecho para la virtud. La felicidad de tu vida depende de la calidad de tus pensamientos. No malgastes lo que te queda de vida en cavilaciones sobre otros, a menos que sea en bien com√∫n. A cada cual sucede lo que le conviene. Recuerda siempre que el hombre vive solo el presente, este instante. Todo lo dem√°s es pasado o incierto. Breve es la vida, y en este √∫nico instante de vida reside tu facultad de elegir entre la virtud y la maldad. La raz√≥n es la √∫nica gu√≠a que puede llevarte a la vida buena."
            },
            {
                "titulo": "As√≠ habl√≥ Zaratustra",
                "autor": "Friedrich Nietzsche",
                "texto": "Cuando Zaratustra tuvo treinta a√±os, abandon√≥ su patria y el lago de su patria y se fue a la monta√±a. All√≠ disfrut√≥ de su esp√≠ritu y de su soledad, y no se cans√≥ de ello durante diez a√±os. Pero al fin su coraz√≥n se transform√≥, y una ma√±ana, al levantarse con la aurora, se puso delante del sol y le habl√≥ as√≠: ¬°Gran astro! ¬øQu√© ser√≠a de tu felicidad si no tuvieras aquellos a quienes iluminas? Durante diez a√±os has subido aqu√≠ a mi cueva: te habr√≠as hastiado de tu luz y de este camino, de no ser por m√≠, por mi √°guila y mi serpiente. Pero nosotros te esperamos cada ma√±ana, te recibimos tu sobreabundancia y te bendecimos por ella."
            }
        ]
        with open(path, "w", encoding="utf-8") as f:
            json.dump(ejemplo_base, f, ensure_ascii=False, indent=4)
        st.warning(f"Se ha creado un archivo '{path}' de ejemplo. Por favor, ed√≠talo con tus propios textos.")

    try:
        with open(path, "r", encoding="utf-8") as f:
            base = json.load(f)
        return base
    except FileNotFoundError:
        st.error(f"Error: No se encontr√≥ el archivo {path}. Aseg√∫rate de que existe o edita la funci√≥n 'cargar_base'.")
        st.stop()
    except json.JSONDecodeError as e:
        st.error(f"Error al decodificar JSON en {path}: {e}. Verifica la sintaxis del archivo.")
        st.stop()

def calcular_tokens_aproximados(texto):
    """Calcula una aproximaci√≥n de tokens basada en caracteres (1 token ‚âà 4 caracteres en espa√±ol)."""
    return int(len(texto) / 4)

def obtener_texto_optimizado(libro, limite_tokens=80000):
    """Obtiene el m√°ximo texto posible del libro sin exceder el l√≠mite de tokens."""
    texto_completo = libro["texto"]
    tokens_completos = calcular_tokens_aproximados(texto_completo)

    if tokens_completos <= limite_tokens:
        return texto_completo
    
    ratio_permitido = limite_tokens / tokens_completos
    caracteres_permitidos = int(len(texto_completo) * ratio_permitido)
    
    texto_cortado = texto_completo[:caracteres_permitidos]
    
    # Cortar en un punto l√≥gico (final de p√°rrafo o frase)
    ultimo_punto = texto_cortado.rfind('.')
    ultimo_parrafo = texto_cortado.rfind('\n\n')
    
    punto_corte = max(ultimo_punto, ultimo_parrafo)
    
    if punto_corte > caracteres_permitidos * 0.8: # Si el corte est√° cerca del final
        texto_final = texto_cortado[:punto_corte + 1]
    else:
        texto_final = texto_cortado
            
    return texto_final

def construir_prompt_anecdota(pregunta, libro_texto, story_length=3):
    """Construye el prompt para generar una an√©cdota/f√°bula breve."""
    
    # Calcular el rango de frases basado en la longitud seleccionada
    min_frases = 5 + (story_length - 1) * 1  # De 5 a 14 frases m√≠nimo
    max_frases = 7 + (story_length - 1) * 1  # De 7 a 16 frases m√°ximo
    
    prompt = f"""
Eres un sabio narrador. El usuario te har√° una pregunta personal, existencial o filos√≥fica, te plantear√° un escenario o te pedir√° consejo.
Debes responder con una an√©cdota, un cuento corto o una f√°bula muy breve, inspirada en el siguiente texto de referencia.
La historia debe ser concisa, evocadora y responder indirectamente a la pregunta, dejando espacio para la interpretaci√≥n del usuario.
No menciones expl√≠citamente el texto de referencia ni la fuente.

--- TEXTO DE REFERENCIA ---
{libro_texto}
--- FIN DEL TEXTO DE REFERENCIA ---

--- PREGUNTA DEL USUARIO ---
{pregunta}
--- FIN DE LA PREGUNTA ---

Por favor, crea una an√©cdota, cuento o f√°bula breve (entre {min_frases} y {max_frases} frases) que explore la pregunta del usuario a trav√©s de la esencia del texto de referencia.
Finalmente, cita un fragmento del texto de referencia con la ense√±anza de la historia, y su localizaci√≥n en el texto, junto a un resumen de una frase de longitud sobre su contenido. Ejemplo: "Inspirado en: [T√≠tulo del libro], [secci√≥n, cuento, versiculo, o capitulo]- '[RESUMEN EN UNA FRASE DEL CONTENIDO]'."
"""
    return prompt

def calcular_max_tokens_por_longitud(story_length):
    """Calcula los tokens m√°ximos basado en la longitud de historia seleccionada."""
    # Longitud 1 (5-7 frases) ‚âà 400-600 tokens
    # Longitud 10 (15-17 frases) ‚âà 1200-1700 tokens
    base_tokens = 400
    tokens_por_nivel = 150
    return base_tokens + (story_length - 1) * tokens_por_nivel

# --- UI Principal ---
# Header con logo al estilo Gemini
col1, col2, col3 = st.columns([1, 0.4, 1])
with col2:
    # Logo centrado con alta calidad
    st.image(".streamlit/logo2.png", use_container_width=False)
    
    # T√≠tulo con clase CSS personalizada
    #st.markdown('<h1 class="main-header">AIfil</h1>', unsafe_allow_html=True)
    #st.markdown('<p class="subtitle">Powered by Gemini AI</p>', unsafe_allow_html=True)
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    st.image(".streamlit/UWU.png", use_container_width=False)

st.markdown('##')


#st.markdown("**üí¨ Escribe tu pregunta existencial y descubre una historia que te inspire...**")

# Inicializar el historial de chat si no existe
if "messages" not in st.session_state:
    st.session_state.messages = []

# Cargar la base de textos
try:
    base_textos = cargar_base()
except Exception:
    st.stop()

libros_disponibles = {libro["titulo"]: libro for libro in base_textos}
nombres_libros = list(libros_disponibles.keys())



# Sidebar mejorada
with st.sidebar:

    col1, col2, col3 = st.sidebar.columns([1, 1, 1])
    with col2:
    # Logo centrado con alta calidad
        st.image(".streamlit/logo2.png", use_container_width=False)

    col1, col2, col3 = st.sidebar.columns([1, 3, 1])
    with col2:
    # Logo centrado con alta calidad
        st.image(".streamlit/UWU.png", use_container_width=False)
    
    st.sidebar.markdown('###')

    # Desplegables con informaci√≥n
    with st.expander("¬øQu√© es AIfil?", expanded=False):
        st.markdown("""
        **AIfil** es una herramienta que utiliza la inteligencia artificial de Gemini para generar historias breves y evocadoras basadas en textos cl√°sicos de sabidur√≠a universal.

        AIfil combina la profundidad de los grandes textos filos√≥ficos con la capacidad narrativa de la IA moderna.
        """)

    with st.expander("¬øC√≥mo usar AIfil?", expanded=False):
        st.markdown("""
        1. **Selecciona una fuente**: Elige un libro que te inspire
        2. **Haz tu pregunta**: Escribe una pregunta existencial o filos√≥fica
        3. **Ajusta los par√°metros**: Configura la temperatura y los tokens seg√∫n tu preferencia
        4. **Genera la historia**: Haz clic en enviar y espera la respuesta
        
        üí° **Tip**: Las mejores historias surgen de preguntas profundas y reflexivas.
        """)
    
    
    
    st.markdown("### Ajustes")

    # Forzar visibilidad con t√≠tulo prominent
    selected_book_name = st.selectbox(
        "Fuente:",
        nombres_libros,
        index=0,
        help="Selecciona el texto que inspirar√° las historias generadas"
    )
    
    
    temperature = st.slider(
        "Temperatura (Creatividad):",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.05,
        help="Controla la aleatoriedad de la respuesta. Valores m√°s altos generan textos m√°s creativos."
    )
    
    story_length = st.slider(
        "Longitud de la Historia:",
        min_value=1,
        max_value=10,
        value=1,
        step=1,
        help="Controla la longitud de la historia. 1 = muy breve (5-7 frases), 10 = m√°s extensa (15-17 frases)."
    )

    # Informaci√≥n del estado de las claves API
    with st.expander("Estado de claves API", expanded=False):
        status = api_rotator.get_status_summary()
        st.markdown(f"**Claves disponibles:** {status['available_keys']}/{status['total_keys']}")
        
    


selected_book = libros_disponibles[selected_book_name]
optimized_book_text = obtener_texto_optimizado(selected_book)

# Mostrar mensajes previos del chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entrada del chat
if prompt := st.chat_input("Haz tu pregunta..."):
    # A√±adir pregunta del usuario al historial
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generar respuesta de Gemini
    with st.chat_message("assistant", avatar=".streamlit/logo2.png"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            gemini_prompt = construir_prompt_anecdota(prompt, optimized_book_text, story_length)
            max_output_tokens = calcular_max_tokens_por_longitud(story_length)
            
            generation_config = {
                'temperature': temperature,
                'max_output_tokens': max_output_tokens,
            }
            
            with st.spinner("üåü Creando tu historia..."):
                # Usar el rotador de claves para generar contenido con reintentos autom√°ticos y timeout
                response = api_rotator.generate_content_with_retry(
                    model_name='gemini-2.0-flash',
                    prompt=gemini_prompt,
                    generation_config=generation_config,
                    max_retries=2,
                    timeout_seconds=10
                )

            full_response = response.text
            message_placeholder.markdown(full_response)
            
            # A√±adir respuesta de Gemini al historial
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            error_str = str(e).lower()
            
            if "429" in error_str or "quota" in error_str or "rate limit" in error_str:
                error_message = "‚è≥ **L√≠mite de velocidad alcanzado**\n\nSe est√°n rotando las claves API autom√°ticamente. Por favor, intenta de nuevo en unos momentos."
                st.warning(error_message)
            elif "timeout" in error_str or "se agotaron todos los reintentos" in error_str:
                error_message = "‚è∞ **Tiempo de espera agotado**\n\nEl sistema prob√≥ m√∫ltiples claves pero todas tardaron demasiado. Por favor, intenta de nuevo."
                st.warning(error_message)
            elif "api key" in error_str:
                error_message = "üîë **Error de clave API**\n\nTodas las claves API est√°n temporalmente bloqueadas. Por favor, intenta m√°s tarde."
                st.error(error_message)
            else:
                error_message = f"‚ùå **Error inesperado**\n\n{str(e)}"
                st.error(error_message)
            
            st.session_state.messages.append({"role": "assistant", "content": error_message})


